{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.random.set_seed(1234)\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(1234)\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485152</td>\n",
       "      <td>-0.268583</td>\n",
       "      <td>-0.507123</td>\n",
       "      <td>0.513626</td>\n",
       "      <td>-0.233606</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>-0.343838</td>\n",
       "      <td>0.243563</td>\n",
       "      <td>0.286981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099946</td>\n",
       "      <td>0.081991</td>\n",
       "      <td>0.090898</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.121411</td>\n",
       "      <td>-0.015282</td>\n",
       "      <td>0.080368</td>\n",
       "      <td>-0.118612</td>\n",
       "      <td>-0.045980</td>\n",
       "      <td>-0.011813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.145042</td>\n",
       "      <td>-0.353461</td>\n",
       "      <td>-0.183404</td>\n",
       "      <td>-0.139457</td>\n",
       "      <td>-0.245284</td>\n",
       "      <td>-0.129782</td>\n",
       "      <td>0.252421</td>\n",
       "      <td>-0.349743</td>\n",
       "      <td>0.220273</td>\n",
       "      <td>0.805366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155708</td>\n",
       "      <td>-0.099277</td>\n",
       "      <td>0.057628</td>\n",
       "      <td>0.085873</td>\n",
       "      <td>-0.022387</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>0.083693</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.141666</td>\n",
       "      <td>0.119717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.173621</td>\n",
       "      <td>-0.210423</td>\n",
       "      <td>-0.200054</td>\n",
       "      <td>-0.314970</td>\n",
       "      <td>0.105571</td>\n",
       "      <td>-0.199907</td>\n",
       "      <td>-0.110117</td>\n",
       "      <td>0.278727</td>\n",
       "      <td>-0.162158</td>\n",
       "      <td>0.274271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159069</td>\n",
       "      <td>-0.130396</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.038919</td>\n",
       "      <td>-0.133981</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.069247</td>\n",
       "      <td>-0.060637</td>\n",
       "      <td>-0.157007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396161</td>\n",
       "      <td>-0.094074</td>\n",
       "      <td>-0.584406</td>\n",
       "      <td>0.188463</td>\n",
       "      <td>0.424859</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>-0.447740</td>\n",
       "      <td>-0.320468</td>\n",
       "      <td>0.039108</td>\n",
       "      <td>-0.089374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037839</td>\n",
       "      <td>-0.076964</td>\n",
       "      <td>0.109443</td>\n",
       "      <td>-0.010140</td>\n",
       "      <td>0.230267</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>-0.201982</td>\n",
       "      <td>0.065821</td>\n",
       "      <td>-0.019499</td>\n",
       "      <td>-0.062648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.427098</td>\n",
       "      <td>-0.211032</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.211464</td>\n",
       "      <td>-0.214811</td>\n",
       "      <td>-0.171968</td>\n",
       "      <td>-0.411327</td>\n",
       "      <td>-0.091980</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>-0.117016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249445</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>-0.207839</td>\n",
       "      <td>0.081620</td>\n",
       "      <td>0.099522</td>\n",
       "      <td>-0.040893</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.224720</td>\n",
       "      <td>0.039183</td>\n",
       "      <td>-0.006331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.657660</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>-0.639379</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>0.133790</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>0.578221</td>\n",
       "      <td>-0.173464</td>\n",
       "      <td>-0.132391</td>\n",
       "      <td>-0.040307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125742</td>\n",
       "      <td>0.031025</td>\n",
       "      <td>-0.127581</td>\n",
       "      <td>-0.092240</td>\n",
       "      <td>-0.084597</td>\n",
       "      <td>-0.005976</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>-0.047780</td>\n",
       "      <td>0.064984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.326346</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>-0.013171</td>\n",
       "      <td>0.085992</td>\n",
       "      <td>0.112580</td>\n",
       "      <td>0.142915</td>\n",
       "      <td>0.355098</td>\n",
       "      <td>-0.170318</td>\n",
       "      <td>-0.290209</td>\n",
       "      <td>-0.250151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>-0.073200</td>\n",
       "      <td>-0.107384</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.170823</td>\n",
       "      <td>0.098070</td>\n",
       "      <td>0.042203</td>\n",
       "      <td>-0.056280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.211847</td>\n",
       "      <td>-0.293808</td>\n",
       "      <td>-0.109366</td>\n",
       "      <td>-0.112704</td>\n",
       "      <td>0.082216</td>\n",
       "      <td>-0.011944</td>\n",
       "      <td>0.378527</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>-0.203169</td>\n",
       "      <td>0.315890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081705</td>\n",
       "      <td>-0.005122</td>\n",
       "      <td>0.074399</td>\n",
       "      <td>-0.203843</td>\n",
       "      <td>-0.166209</td>\n",
       "      <td>-0.082116</td>\n",
       "      <td>-0.042335</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>-0.022748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.334557</td>\n",
       "      <td>-0.315646</td>\n",
       "      <td>-0.028062</td>\n",
       "      <td>0.103931</td>\n",
       "      <td>-0.044717</td>\n",
       "      <td>0.070572</td>\n",
       "      <td>0.359556</td>\n",
       "      <td>-0.061466</td>\n",
       "      <td>-0.081412</td>\n",
       "      <td>0.241167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024980</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.203389</td>\n",
       "      <td>0.241135</td>\n",
       "      <td>-0.139370</td>\n",
       "      <td>-0.015324</td>\n",
       "      <td>-0.123194</td>\n",
       "      <td>-0.093305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.255669</td>\n",
       "      <td>-0.362325</td>\n",
       "      <td>-0.069902</td>\n",
       "      <td>0.227747</td>\n",
       "      <td>-0.671268</td>\n",
       "      <td>0.428390</td>\n",
       "      <td>0.260220</td>\n",
       "      <td>-0.071838</td>\n",
       "      <td>0.043992</td>\n",
       "      <td>0.106095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019813</td>\n",
       "      <td>-0.084160</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.067466</td>\n",
       "      <td>-0.131197</td>\n",
       "      <td>0.134438</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>0.055404</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>-0.081546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.485152 -0.268583 -0.507123  0.513626 -0.233606  0.030368  0.058925   \n",
       "1    -0.145042 -0.353461 -0.183404 -0.139457 -0.245284 -0.129782  0.252421   \n",
       "2    -0.173621 -0.210423 -0.200054 -0.314970  0.105571 -0.199907 -0.110117   \n",
       "3     0.396161 -0.094074 -0.584406  0.188463  0.424859  0.632022 -0.447740   \n",
       "4    -0.427098 -0.211032  0.043938  0.211464 -0.214811 -0.171968 -0.411327   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1998  0.657660  0.729199 -0.639379  0.014235  0.133790 -0.012341  0.578221   \n",
       "1999 -0.326346 -0.283518 -0.013171  0.085992  0.112580  0.142915  0.355098   \n",
       "2000 -0.211847 -0.293808 -0.109366 -0.112704  0.082216 -0.011944  0.378527   \n",
       "2001 -0.334557 -0.315646 -0.028062  0.103931 -0.044717  0.070572  0.359556   \n",
       "2002 -0.255669 -0.362325 -0.069902  0.227747 -0.671268  0.428390  0.260220   \n",
       "\n",
       "             7         8         9  ...        36        37        38  \\\n",
       "0    -0.343838  0.243563  0.286981  ... -0.099946  0.081991  0.090898   \n",
       "1    -0.349743  0.220273  0.805366  ...  0.155708 -0.099277  0.057628   \n",
       "2     0.278727 -0.162158  0.274271  ...  0.159069 -0.130396 -0.007525   \n",
       "3    -0.320468  0.039108 -0.089374  ...  0.037839 -0.076964  0.109443   \n",
       "4    -0.091980  0.044690 -0.117016  ... -0.249445  0.027913 -0.207839   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1998 -0.173464 -0.132391 -0.040307  ...  0.125742  0.031025 -0.127581   \n",
       "1999 -0.170318 -0.290209 -0.250151  ...  0.018072 -0.073200 -0.107384   \n",
       "2000  0.025612 -0.203169  0.315890  ... -0.081705 -0.005122  0.074399   \n",
       "2001 -0.061466 -0.081412  0.241167  ...  0.024980  0.011861 -0.003077   \n",
       "2002 -0.071838  0.043992  0.106095  ... -0.019813 -0.084160  0.000388   \n",
       "\n",
       "            39        40        41        42        43        44        45  \n",
       "0    -0.017447  0.121411 -0.015282  0.080368 -0.118612 -0.045980 -0.011813  \n",
       "1     0.085873 -0.022387 -0.130570  0.083693  0.089900  0.141666  0.119717  \n",
       "2     0.038919 -0.133981  0.069500  0.077070  0.069247 -0.060637 -0.157007  \n",
       "3    -0.010140  0.230267  0.041187 -0.201982  0.065821 -0.019499 -0.062648  \n",
       "4     0.081620  0.099522 -0.040893  0.002526  0.224720  0.039183 -0.006331  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1998 -0.092240 -0.084597 -0.005976  0.010087  0.086644 -0.047780  0.064984  \n",
       "1999  0.037324  0.023242  0.009963  0.170823  0.098070  0.042203 -0.056280  \n",
       "2000 -0.203843 -0.166209 -0.082116 -0.042335  0.021290  0.068202 -0.022748  \n",
       "2001  0.053481  0.203389  0.241135 -0.139370 -0.015324 -0.123194 -0.093305  \n",
       "2002 -0.067466 -0.131197  0.134438 -0.030751  0.055404 -0.033788 -0.081546  \n",
       "\n",
       "[2003 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_df = pd.read_csv(\"pca_3.csv\",index_col=[0])\n",
    "PCA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inpatient.number</th>\n",
       "      <th>Atorvastatin calcium tablet</th>\n",
       "      <th>Benazepril hydrochloride tablet</th>\n",
       "      <th>Deslanoside injection</th>\n",
       "      <th>Digoxin tablet</th>\n",
       "      <th>Dobutamine hydrochloride injection</th>\n",
       "      <th>Furosemide injection</th>\n",
       "      <th>Furosemide tablet</th>\n",
       "      <th>Hydrochlorothiazide tablet</th>\n",
       "      <th>Isoprenaline Hydrochloride injection</th>\n",
       "      <th>Isosorbide Mononitrate Sustained Release tablet</th>\n",
       "      <th>Meglumine Adenosine Cyclophosphate for injection</th>\n",
       "      <th>Milrinone injection</th>\n",
       "      <th>Nitroglycerin injection</th>\n",
       "      <th>Shenfu injection</th>\n",
       "      <th>Spironolactone tablet</th>\n",
       "      <th>Torasemide tablet</th>\n",
       "      <th>Valsartan Dispersible tablet</th>\n",
       "      <th>sulfotanshinone sodium injection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>722128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>723327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>723617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>724385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725509</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>870258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>870646</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>879601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>905163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>905720</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      inpatient.number  Atorvastatin calcium tablet  \\\n",
       "0               722128                            0   \n",
       "1               723327                            1   \n",
       "2               723617                            1   \n",
       "3               724385                            0   \n",
       "4               725509                            0   \n",
       "...                ...                          ...   \n",
       "1998            870258                            0   \n",
       "1999            870646                            0   \n",
       "2000            879601                            1   \n",
       "2001            905163                            0   \n",
       "2002            905720                            1   \n",
       "\n",
       "      Benazepril hydrochloride tablet  Deslanoside injection  Digoxin tablet  \\\n",
       "0                                   0                      1               1   \n",
       "1                                   0                      1               1   \n",
       "2                                   0                      0               0   \n",
       "3                                   0                      1               1   \n",
       "4                                   0                      1               1   \n",
       "...                               ...                    ...             ...   \n",
       "1998                                0                      0               0   \n",
       "1999                                1                      0               0   \n",
       "2000                                1                      1               1   \n",
       "2001                                1                      1               1   \n",
       "2002                                0                      0               1   \n",
       "\n",
       "      Dobutamine hydrochloride injection  Furosemide injection  \\\n",
       "0                                      0                     1   \n",
       "1                                      0                     1   \n",
       "2                                      0                     1   \n",
       "3                                      0                     1   \n",
       "4                                      0                     1   \n",
       "...                                  ...                   ...   \n",
       "1998                                   1                     1   \n",
       "1999                                   0                     1   \n",
       "2000                                   0                     1   \n",
       "2001                                   0                     1   \n",
       "2002                                   0                     1   \n",
       "\n",
       "      Furosemide tablet  Hydrochlorothiazide tablet  \\\n",
       "0                     1                           0   \n",
       "1                     1                           0   \n",
       "2                     0                           0   \n",
       "3                     1                           0   \n",
       "4                     1                           0   \n",
       "...                 ...                         ...   \n",
       "1998                  1                           0   \n",
       "1999                  1                           0   \n",
       "2000                  1                           0   \n",
       "2001                  0                           0   \n",
       "2002                  1                           0   \n",
       "\n",
       "      Isoprenaline Hydrochloride injection  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "...                                    ...   \n",
       "1998                                     0   \n",
       "1999                                     0   \n",
       "2000                                     0   \n",
       "2001                                     0   \n",
       "2002                                     0   \n",
       "\n",
       "      Isosorbide Mononitrate Sustained Release tablet  \\\n",
       "0                                                   0   \n",
       "1                                                   1   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "1998                                                0   \n",
       "1999                                                0   \n",
       "2000                                                0   \n",
       "2001                                                0   \n",
       "2002                                                0   \n",
       "\n",
       "      Meglumine Adenosine Cyclophosphate for injection  Milrinone injection  \\\n",
       "0                                                    0                    1   \n",
       "1                                                    0                    1   \n",
       "2                                                    0                    0   \n",
       "3                                                    0                    1   \n",
       "4                                                    0                    1   \n",
       "...                                                ...                  ...   \n",
       "1998                                                 0                    1   \n",
       "1999                                                 0                    0   \n",
       "2000                                                 1                    1   \n",
       "2001                                                 1                    1   \n",
       "2002                                                 1                    0   \n",
       "\n",
       "      Nitroglycerin injection  Shenfu injection  Spironolactone tablet  \\\n",
       "0                           0                 0                      1   \n",
       "1                           0                 0                      1   \n",
       "2                           0                 0                      1   \n",
       "3                           0                 0                      1   \n",
       "4                           0                 1                      1   \n",
       "...                       ...               ...                    ...   \n",
       "1998                        1                 0                      1   \n",
       "1999                        1                 0                      1   \n",
       "2000                        1                 0                      1   \n",
       "2001                        0                 1                      1   \n",
       "2002                        0                 0                      1   \n",
       "\n",
       "      Torasemide tablet  Valsartan Dispersible tablet  \\\n",
       "0                     0                             0   \n",
       "1                     1                             1   \n",
       "2                     0                             0   \n",
       "3                     0                             0   \n",
       "4                     1                             0   \n",
       "...                 ...                           ...   \n",
       "1998                  0                             1   \n",
       "1999                  0                             0   \n",
       "2000                  0                             0   \n",
       "2001                  1                             0   \n",
       "2002                  0                             0   \n",
       "\n",
       "      sulfotanshinone sodium injection  \n",
       "0                                    0  \n",
       "1                                    0  \n",
       "2                                    0  \n",
       "3                                    0  \n",
       "4                                    0  \n",
       "...                                ...  \n",
       "1998                                 0  \n",
       "1999                                 0  \n",
       "2000                                 0  \n",
       "2001                                 0  \n",
       "2002                                 1  \n",
       "\n",
       "[2003 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"drug_onehot_latest.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['inpatient.number'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Atorvastatin calcium tablet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for : Atorvastatin calcium tablet\n"
     ]
    }
   ],
   "source": [
    "print(\"Running for :\",q)\n",
    "trainer = pd.concat([PCA_df, df[q]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>Atorvastatin calcium tablet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.485152</td>\n",
       "      <td>-0.268583</td>\n",
       "      <td>-0.507123</td>\n",
       "      <td>0.513626</td>\n",
       "      <td>-0.233606</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.058925</td>\n",
       "      <td>-0.343838</td>\n",
       "      <td>0.243563</td>\n",
       "      <td>0.286981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081991</td>\n",
       "      <td>0.090898</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.121411</td>\n",
       "      <td>-0.015282</td>\n",
       "      <td>0.080368</td>\n",
       "      <td>-0.118612</td>\n",
       "      <td>-0.045980</td>\n",
       "      <td>-0.011813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.145042</td>\n",
       "      <td>-0.353461</td>\n",
       "      <td>-0.183404</td>\n",
       "      <td>-0.139457</td>\n",
       "      <td>-0.245284</td>\n",
       "      <td>-0.129782</td>\n",
       "      <td>0.252421</td>\n",
       "      <td>-0.349743</td>\n",
       "      <td>0.220273</td>\n",
       "      <td>0.805366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099277</td>\n",
       "      <td>0.057628</td>\n",
       "      <td>0.085873</td>\n",
       "      <td>-0.022387</td>\n",
       "      <td>-0.130570</td>\n",
       "      <td>0.083693</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.141666</td>\n",
       "      <td>0.119717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.173621</td>\n",
       "      <td>-0.210423</td>\n",
       "      <td>-0.200054</td>\n",
       "      <td>-0.314970</td>\n",
       "      <td>0.105571</td>\n",
       "      <td>-0.199907</td>\n",
       "      <td>-0.110117</td>\n",
       "      <td>0.278727</td>\n",
       "      <td>-0.162158</td>\n",
       "      <td>0.274271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130396</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.038919</td>\n",
       "      <td>-0.133981</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.069247</td>\n",
       "      <td>-0.060637</td>\n",
       "      <td>-0.157007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396161</td>\n",
       "      <td>-0.094074</td>\n",
       "      <td>-0.584406</td>\n",
       "      <td>0.188463</td>\n",
       "      <td>0.424859</td>\n",
       "      <td>0.632022</td>\n",
       "      <td>-0.447740</td>\n",
       "      <td>-0.320468</td>\n",
       "      <td>0.039108</td>\n",
       "      <td>-0.089374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076964</td>\n",
       "      <td>0.109443</td>\n",
       "      <td>-0.010140</td>\n",
       "      <td>0.230267</td>\n",
       "      <td>0.041187</td>\n",
       "      <td>-0.201982</td>\n",
       "      <td>0.065821</td>\n",
       "      <td>-0.019499</td>\n",
       "      <td>-0.062648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.427098</td>\n",
       "      <td>-0.211032</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.211464</td>\n",
       "      <td>-0.214811</td>\n",
       "      <td>-0.171968</td>\n",
       "      <td>-0.411327</td>\n",
       "      <td>-0.091980</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>-0.117016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>-0.207839</td>\n",
       "      <td>0.081620</td>\n",
       "      <td>0.099522</td>\n",
       "      <td>-0.040893</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.224720</td>\n",
       "      <td>0.039183</td>\n",
       "      <td>-0.006331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.657660</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>-0.639379</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>0.133790</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>0.578221</td>\n",
       "      <td>-0.173464</td>\n",
       "      <td>-0.132391</td>\n",
       "      <td>-0.040307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031025</td>\n",
       "      <td>-0.127581</td>\n",
       "      <td>-0.092240</td>\n",
       "      <td>-0.084597</td>\n",
       "      <td>-0.005976</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>-0.047780</td>\n",
       "      <td>0.064984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.326346</td>\n",
       "      <td>-0.283518</td>\n",
       "      <td>-0.013171</td>\n",
       "      <td>0.085992</td>\n",
       "      <td>0.112580</td>\n",
       "      <td>0.142915</td>\n",
       "      <td>0.355098</td>\n",
       "      <td>-0.170318</td>\n",
       "      <td>-0.290209</td>\n",
       "      <td>-0.250151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073200</td>\n",
       "      <td>-0.107384</td>\n",
       "      <td>0.037324</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.170823</td>\n",
       "      <td>0.098070</td>\n",
       "      <td>0.042203</td>\n",
       "      <td>-0.056280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.211847</td>\n",
       "      <td>-0.293808</td>\n",
       "      <td>-0.109366</td>\n",
       "      <td>-0.112704</td>\n",
       "      <td>0.082216</td>\n",
       "      <td>-0.011944</td>\n",
       "      <td>0.378527</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>-0.203169</td>\n",
       "      <td>0.315890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005122</td>\n",
       "      <td>0.074399</td>\n",
       "      <td>-0.203843</td>\n",
       "      <td>-0.166209</td>\n",
       "      <td>-0.082116</td>\n",
       "      <td>-0.042335</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>-0.022748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-0.334557</td>\n",
       "      <td>-0.315646</td>\n",
       "      <td>-0.028062</td>\n",
       "      <td>0.103931</td>\n",
       "      <td>-0.044717</td>\n",
       "      <td>0.070572</td>\n",
       "      <td>0.359556</td>\n",
       "      <td>-0.061466</td>\n",
       "      <td>-0.081412</td>\n",
       "      <td>0.241167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>-0.003077</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.203389</td>\n",
       "      <td>0.241135</td>\n",
       "      <td>-0.139370</td>\n",
       "      <td>-0.015324</td>\n",
       "      <td>-0.123194</td>\n",
       "      <td>-0.093305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>-0.255669</td>\n",
       "      <td>-0.362325</td>\n",
       "      <td>-0.069902</td>\n",
       "      <td>0.227747</td>\n",
       "      <td>-0.671268</td>\n",
       "      <td>0.428390</td>\n",
       "      <td>0.260220</td>\n",
       "      <td>-0.071838</td>\n",
       "      <td>0.043992</td>\n",
       "      <td>0.106095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084160</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>-0.067466</td>\n",
       "      <td>-0.131197</td>\n",
       "      <td>0.134438</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>0.055404</td>\n",
       "      <td>-0.033788</td>\n",
       "      <td>-0.081546</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.485152 -0.268583 -0.507123  0.513626 -0.233606  0.030368  0.058925   \n",
       "1    -0.145042 -0.353461 -0.183404 -0.139457 -0.245284 -0.129782  0.252421   \n",
       "2    -0.173621 -0.210423 -0.200054 -0.314970  0.105571 -0.199907 -0.110117   \n",
       "3     0.396161 -0.094074 -0.584406  0.188463  0.424859  0.632022 -0.447740   \n",
       "4    -0.427098 -0.211032  0.043938  0.211464 -0.214811 -0.171968 -0.411327   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1998  0.657660  0.729199 -0.639379  0.014235  0.133790 -0.012341  0.578221   \n",
       "1999 -0.326346 -0.283518 -0.013171  0.085992  0.112580  0.142915  0.355098   \n",
       "2000 -0.211847 -0.293808 -0.109366 -0.112704  0.082216 -0.011944  0.378527   \n",
       "2001 -0.334557 -0.315646 -0.028062  0.103931 -0.044717  0.070572  0.359556   \n",
       "2002 -0.255669 -0.362325 -0.069902  0.227747 -0.671268  0.428390  0.260220   \n",
       "\n",
       "             7         8         9  ...        37        38        39  \\\n",
       "0    -0.343838  0.243563  0.286981  ...  0.081991  0.090898 -0.017447   \n",
       "1    -0.349743  0.220273  0.805366  ... -0.099277  0.057628  0.085873   \n",
       "2     0.278727 -0.162158  0.274271  ... -0.130396 -0.007525  0.038919   \n",
       "3    -0.320468  0.039108 -0.089374  ... -0.076964  0.109443 -0.010140   \n",
       "4    -0.091980  0.044690 -0.117016  ...  0.027913 -0.207839  0.081620   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1998 -0.173464 -0.132391 -0.040307  ...  0.031025 -0.127581 -0.092240   \n",
       "1999 -0.170318 -0.290209 -0.250151  ... -0.073200 -0.107384  0.037324   \n",
       "2000  0.025612 -0.203169  0.315890  ... -0.005122  0.074399 -0.203843   \n",
       "2001 -0.061466 -0.081412  0.241167  ...  0.011861 -0.003077  0.053481   \n",
       "2002 -0.071838  0.043992  0.106095  ... -0.084160  0.000388 -0.067466   \n",
       "\n",
       "            40        41        42        43        44        45  \\\n",
       "0     0.121411 -0.015282  0.080368 -0.118612 -0.045980 -0.011813   \n",
       "1    -0.022387 -0.130570  0.083693  0.089900  0.141666  0.119717   \n",
       "2    -0.133981  0.069500  0.077070  0.069247 -0.060637 -0.157007   \n",
       "3     0.230267  0.041187 -0.201982  0.065821 -0.019499 -0.062648   \n",
       "4     0.099522 -0.040893  0.002526  0.224720  0.039183 -0.006331   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "1998 -0.084597 -0.005976  0.010087  0.086644 -0.047780  0.064984   \n",
       "1999  0.023242  0.009963  0.170823  0.098070  0.042203 -0.056280   \n",
       "2000 -0.166209 -0.082116 -0.042335  0.021290  0.068202 -0.022748   \n",
       "2001  0.203389  0.241135 -0.139370 -0.015324 -0.123194 -0.093305   \n",
       "2002 -0.131197  0.134438 -0.030751  0.055404 -0.033788 -0.081546   \n",
       "\n",
       "      Atorvastatin calcium tablet  \n",
       "0                               0  \n",
       "1                               1  \n",
       "2                               1  \n",
       "3                               0  \n",
       "4                               0  \n",
       "...                           ...  \n",
       "1998                            0  \n",
       "1999                            0  \n",
       "2000                            1  \n",
       "2001                            0  \n",
       "2002                            1  \n",
       "\n",
       "[2003 rows x 47 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "0    1185\n",
      "1    1185\n",
      "Name: Atorvastatin calcium tablet, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_class_0, count_class_1 = trainer[q].value_counts()[0], trainer[q].value_counts()[1]\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = trainer[trainer[q] == 0]\n",
    "df_class_1 = trainer[trainer[q] == 1]\n",
    "if count_class_0>count_class_1:\n",
    "    df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "    df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "else:\n",
    "    df_class_0_over = df_class_0.sample(count_class_1,replace=True)\n",
    "    df_test_over = pd.concat([df_class_0_over, df_class_1], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over[q].value_counts())\n",
    "X = df_test_over.drop(q,axis='columns')\n",
    "y = df_test_over[q]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "weights = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import History \n",
    "# history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=4, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.65      0.72       237\n",
      "           1       0.70      0.84      0.76       237\n",
      "\n",
      "    accuracy                           0.74       474\n",
      "   macro avg       0.75      0.74      0.74       474\n",
      "weighted avg       0.75      0.74      0.74       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, input_dim=46, activation='relu'),\n",
    "    keras.layers.Dense(15, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "if weights == -1:\n",
    "    his = model.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=50,verbose = 0)\n",
    "else:\n",
    "    his = model.fit(X_train, y_train, epochs=50, class_weight = weights)\n",
    "\n",
    "model.evaluate(X_test, y_test,verbose = 0)\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "y_preds = np.round(y_preds)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 1s 5ms/step - loss: 2.0021 - accuracy: 0.5248 - val_loss: 1.4572 - val_accuracy: 0.6055\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 1.1767 - accuracy: 0.5675 - val_loss: 0.9604 - val_accuracy: 0.6118\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.8578 - accuracy: 0.5554 - val_loss: 0.7812 - val_accuracy: 0.5717\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.7468 - accuracy: 0.5585 - val_loss: 0.7216 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.7106 - accuracy: 0.5058 - val_loss: 0.7025 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6991 - accuracy: 0.5000 - val_loss: 0.6965 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6954 - accuracy: 0.4794 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.4958 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6936 - accuracy: 0.4958 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4895 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4842 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4852 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4863 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4747 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4821 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4821 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4842 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4863 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4852 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4852 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4810 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4810 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4979 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4789 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4937 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4800 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4810 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4662 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4905 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4831 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4831 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4947 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4831 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4916 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4842 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4852 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4757 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "[0.6931481957435608, 0.5]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       237\n",
      "           1       0.00      0.00      0.00       237\n",
      "\n",
      "    accuracy                           0.50       474\n",
      "   macro avg       0.25      0.50      0.33       474\n",
      "weighted avg       0.25      0.50      0.33       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg_model = Sequential()\n",
    "reg_model.add(Dense(128, input_dim=46, activation='relu',  kernel_regularizer='l2'))\n",
    "reg_model.add(Dropout(0.2))\n",
    "reg_model.add(Dense(64, activation='relu',  kernel_regularizer='l2'))\n",
    "reg_model.add(Dense(32, activation='relu',  kernel_regularizer='l2'))\n",
    "# reg_model.add(Dropout(0.2))\n",
    "reg_model.add(Dense(1, activation='sigmoid', kernel_regularizer='l2'))\n",
    "reg_model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = reg_model.fit(X_train, y_train, \n",
    "                            validation_data=(X_test, y_test), \n",
    "                            epochs=100, verbose=1)\n",
    "\n",
    "print(reg_model.evaluate(X_test, y_test))\n",
    "\n",
    "y_preds = reg_model.predict(X_test)\n",
    "y_preds = np.round(y_preds)\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['loss'], label='train')\n",
    "plt.plot(his.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfb7935b20b1350c43bf64077ed6a04b20dae5a2fec5d0927cb7fd3126576106"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
